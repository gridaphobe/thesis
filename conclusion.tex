The goal of this work has been to improve the diagnostic feedback that
compilers provide when a program with no type annotations fails to type-check.
%
To that end, we have made three key contributions that advance the state
of the art in type error diagnosis.

\paragraph{Contribution 1: A Dataset of Novice Type Errors}
Our first contribution was a new dataset of novice interactions with the
\ocaml top-level interpreter (in particular, type errors they
encountered and their fixes).
%
The dataset contains thousands of ill-typed programs written by over one
hundred undergraduate students at UC San Diego, as well as the
subsequent fixes.
%
This is the largest set of novice type errors that we are aware of, has
formed the backbone of our evaluation, and will hopefully be similarly
useful to other researchers in the future.

\paragraph{Contribution 2: Dynamic Witnesses for Static Type Errors}
Second, we presented a novel technique for explaining type errors in
terms of the underlying runtime error the type system prevented.
%
We interleave type inference and execution to search for a witness to
the type error, a set of inputs that would cause the program to crash
at runtime.
%
We borrow the notion of ``holes'' from the automatic testing literature
to avoid spurious witnesses by delaying the selection of a concrete
input until execution has reached a point where we can be sure of its
type.
%
Once our search procedure finds a witness, we compute a full execution
trace that demonstrates how the program would evolve and eventually
crash.
%
We present this trace to the user in an interactive debugger that allows
the user to explore the erroneous computation in a familiar setting.

We proved that our search procedure produces general witnesses, \ie if
we can find a witness the program must be untypeable.
%
We showed empirically that most novice type errors, around 85\%, admit
witnesses, and that the vast majority can be found in under one second
by our search procedure.
%
We also found that students who were given our witnesses were more
likely to correctly explain and fix a type error than students who were
just given \ocaml's error message.
%
Finally, we found that our witnesses can also serve as a localization
method for type errors by treating the stuck term as a sink for typing
constraints and the values contained within it as sources.
%
Our witness-based localizations are substantially more accurate than
\ocaml's errors and competitive with the state of the art.

\paragraph{Contribution 3: Data-Driven Diagnosis of Type Errors}
Finally, we presented a novel technique for localizing type errors based
on observations of past errors and their fixes.
%
We use machine learning to train a classifier that predicts, given a
term from an ill-typed program, whether the term is likely to be changed
in the eventual fix (\ie is that term to blame for the error).
%
Given a new ill-typed program, we run the classifier for all program
terms and use its confidence score to rank the terms by the likelihood
that they should be blamed, selecting only the top three to present to
the user.
%
The classifier makes predictions based solely on the syntax and types of
the term and its immediate parent and children, and, crucially, whether
the term is part of a minimal type error slice.

Our classifier's top-ranked prediction is at least \ToolnameWinSherrloc
percentage points more accurate than the state of the art in type error
localization, and given three predictions it exceeds 90\% accuracy.
%
Furthermore, the classifier can be trained on a modest amount of data;
we obtained our results by training on programs from a single instance
of our undergraduate programming languages course at UC San Diego.
%
This makes us confident that even if our model does not generalize to
programs from other courses (or more broadly, to arbitrary \ocaml
programs), it is quite reasonable for instructors to train models of the
specific errors made by students in their courses.


\section{Future Work}
\label{sec:conc:future-work}
We will conclude this dissertation with a brief discussion of some
exciting future directions for this line of work.

\paragraph{Other Type Systems and Analyses}
This dissertation has focused on improving type errors for typed
functional languages based on the Hindley-Milner type system, but
there are a great many other type systems in use.
%
Thus, one promising direction for future work would be adapting our
techniques to other systems.

Both \tool{NanoMaLy} and \tool{Nate} have been designed to be parametric
in the type system, so in principle it should be straightforward to
adapt them to other languages and type systems.
%
\tool{NanoMaLy}'s use of the type system is mostly confined to the
\forcesym procedure that performs type-checking, thus one would need to
adapt \forcesym to the target type system. If the dynamic semantics of
the target language differ significantly from \ocaml's, one would also
need to replace the evaluation rules, but this is not a significant
burden either. \tool{NanoMaLy}'s evaluation rules are just those of
\ocaml, with a call to \forcesym inserted before every primitive
reduction.
%
In contrast, \tool{Nate} only uses the type system as a source of
features, so one would only need to extract alternative features from
the target type system.

We will next briefly outline a few classes of type systems and how
supporting them might differ from \ocaml.

\subparagraph{Dependent Types}
%
Dependent types~\citep{Bertot2013-ao,Norell2007-cj,Brady2013-nl}
and their close cousins,
refinement types~\citep{Xi1998-we,Dunfield2007-ei,Rondon2008-ea,Swamy2011-rq},
%
allow the programmer to specify complex invariants on their programs and
data, and can statically prevent many runtime errors that \ocaml cannot.
%
These systems have been used to prove
%
the absence out-of-bounds accesses~\citep{Xi1998-we,Rondon2008-ea,Vazou2014-xk},
%
complex data-structure invariants (\eg red-black tree balancing)~\citep{Vazou2014-xk,Kawaguchi2009-cl},
%
security policies~\citep{Bengtson2011-ep,Swamy2011-rq},
%
and even compiler correctness~\citep{Leroy2009-zs}.

As one might expect, it is much harder to prove such properties about
your programs than traditional type-safety, and thus programmers using
such systems may spend much more time investigating type errors.
%
Another consequence of the increased expressiveness is that these
systems generally do not support global type inference as it becomes
undecidable.
%
Thus, the debugging type errors in these systems is more about
\emph{understanding} the error than \emph{localizing} it.
%
A crucial question the programmer must answer is whether the error is a
legitimate bug in her program, or if the type checker simply lacks
enough information to \emph{prove} the program correct.

\tool{NanoMaLy}'s approach to searching for witnesses to type errors
could thus be quite helpful in these systems.
%
The presence of a witness proves that there is an actual bug, while the
absence may suggest that the programmer needs to supply some additional
lemmas to convince the type checker.
%
We have done some preliminary work in this area, showing that dependent
and refinement type signatures can be thought of as generators and
oracles for comprehensive test suites~\citep{Seidel2015-pe}.
%
However, that work only checked that a function satisfies its top-level
signature, we did not search for witnesses to the misuse of other
functions internally.
%
\citet{Petiot2016-fe} present a technique for determining if a proof
failure is due to a legitimate bug or a lack of available information,
but they only evaluate it in the context of first-order, imperative C
programs.
%
It would be very interesting to apply these techniques to dependent
and refinement type systems for functional languages.

\subparagraph{Objects}
%
Objects are a common feature in popular languages like \tool{C++},
\tool{Java}, and \tool{C\#}, offering code reuse via inheritance and
behavioral abstraction via interfaces.
%
A core feature of type systems that support objects is \emph{subtyping},
allowing values of the sub-type to be seamlessly used anywhere values of
the super-type are expected.
%
While these languages are gradually adopting type inference for local
variables, they generally require type annotations for
functions\footnote{\ocaml's own object system is a notable exception
  here.} as the addition of subtyping would force the compiler to guess
the programmer's intent for the input types.
%
For instance, did she want the function to accept objects of a specific
type, or objects of any type that implement a particular interface?

Since the type checker is (generally) not responsible for guessing the
programmer's intent in these systems, we suspect that type error
localization is unlikely to be as serious problem as it is in \ocaml;
however, these languages may still benefit from \tool{NanoMaLy}'s
approach to explaining type errors in terms of runtime errors.
%
Novice users, in particular, may find it easier to understand the error
when presented as a concrete runtime trace.
%
\citet{Bayne2011-cn} demonstrate a tool similar to \tool{NanoMaLy} that
allows programmers to execute ill-typed \tool{Java} programs, though
their aim is user-driven testing of a program in spite of potentially
irrelevant type errors, while ours is automated explanations of
type errors.
%
Furthermore, subtyping adds a similar question of whether the type error
is legitimate or if a function was simply being too conservative in its
input and output types, thus testing may help guide the programmer to a
solution as suggested above.

% \subparagraph{Gradual Types}
% %
% \ES{TODO}

\subparagraph{Information Flow Control}
%
Several authors have proposed type systems for tracking who may access
or modify certain pieces of data, \eg medical records or paper reviews,
in order to ensure confidentiality~\citep{Denning1977-kk,Heintze1998-hu,Myers2000-zd,Pottier2003-et,Stefan2011-cd}.
%
These systems typically associate a security label --- ranging from
simple ``high'' or ``low'' security label, to a set of privileged actors
--- in addition to a type with each object in the system, and ensure
that only actors with sufficient privileges can access restricted
objects.
%
A major complication from traditional type-checking is that \emph{implicit}
information flows must be tracked in addition to explicit flows.
%
While an explicit information flow could be returning a row from a
database table, an implicit flow occurs when the program makes an
observable decision based on a piece of information, \eg by branching
on the value of an object.
%
Implicit flows must be restricted so that malicious users cannot infer
privileged data by carefully constructed inputs to a system;
%
a common tactic is to conservatively propagate security labels via
implicit flows.

As with objects, these type systems do not perform global inference for
the security labels, thus the type checker is not trying to infer the
programmer's intent; however, the implicit flows can create a similar
issue of errors being reported far from their source.
%
Thus, we suspect that these systems could benefit from both approaches
presented in this dissertation: \tool{Nate}'s data-driven localizations
could improve the accuracy of error reports, and \tool{NanoMaLy}'s
witnesses could help explain the errors in terms of the undesirable
leaking of privileged data.

\paragraph{Fixing Type Errors}
Throughout this entire dissertation we have focused on localizing and
explaining type errors, but % many type errors are rather mundane things.
% %
% For example, perhaps we used \ocaml's |+| operator for |int|s rather
% than the |+.| operator for |float|s.
% %
% For these errors in particular
it would be nice to have a tool that
could simply \emph{fix} the error without any user intervention.

In addition to the techniques we mentioned in
\autoref{sec:fixing-type-errors} there is a wealth of existing work in
the broader field of automatic program repair~\citep[see][\S~4, for a
survey]{Le_Goues2013-ag} that we could draw from.
%
A core challenge in program repair is fault localization, \ie
determining where the repair should take place.
%
Thus, \textsc{Nate}'s ability to accurately locate the source of type
errors could provide a strong foundation on top of which to build repair
systems.

Another exciting opportunity for future work would be using machine
learning to predict fixes \emph{in addition} to blame labels.
%
In fact, this could be a very natural extension of our work on
\textsc{Nate}, we can frame it as a classification problem as follows.
%
Given a term from an ill-typed program, that we have identified as a
candidate for blame, we want to predict which local syntactic feature
should be enabled for the corresponding term in the program's fix.
%
For example, in our |sumList| example, we would like the classifier to
predict that the \textsc{Is-Int} feature should be enabled, instead of
the \textsc{Is-[]} feature, in the fix.
%
The problem then becomes a \emph{multi-label} classification problem, as
there are many possible syntactic features to choose from, but these
problems are also well studied in the machine learning literature.

This approach could work nicely for relatively simple fixes like for
|sumList|, or a use of the integer |+| rather than the floating-point
|+.|, \etc, but many fixes will require multiple edits to the program.
%
For example, even adding an extra argument to a function call would be a
multi-edit fix in \ocaml: first we would need to insert a new
application node with the old node as its left child, then we would need
to synthesize a new term for the right child.
%
Clearly, such a fix cannot be generated by \textsc{Nate} as is, since
there are an infinite number of possible terms but a finite number of
labels that we can predict.
%
However, there has been much work in the machine learning community on
models that can generate structured data, most notably
images~\citep{Gregor2015-ra} and text~\citep{Bahdanau2014-gt}, but also
program terms~\citep{Raychev2016-xk,Raychev2014-jv}.
%
A sizable challenge to adopting these techniques is that they tend to
require vast amounts of data to learn a precise model, and we have a
comparatively small amount of type-error data.
%
However, all is not lost.
%
The type-error data allowed us to predict an accurate location for the
fix, but the fix itself should be a type-\emph{correct} program, and
there are an abundance of type-correct programs publicly available in
software repositories like \textsc{GitHub}.
%
So it may be possible to train a precise model of type-correct programs
using publicly available data, and then use that model to generate
structured fixes to ill-typed programs.

% If we cannot confidently predict a syntactic fix, perhaps we can predict
% a type-level fix, \eg the |+| term should be replaced by a term with type
% |float -> float -> float|.

% \ES{investigate using witnesses to drive fixes?}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
