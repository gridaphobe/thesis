\section{Related Work} \label{sec:related}

\toolname is closely related to a number of lines of work on connecting
formal specifications, execution, and automated constraint-based testing. 
Next, we describe the closest lines of work on test-generation and 
situate them with respect to our approach.

\subsection{Model-based Testing}
\label{sec:model-based-testing}
Model-based testing encompasses a broad range of black-box testing tools that
facilitate generating concrete test-cases from an abstract model of the system
under test. These systems generally (though not necessarily) model the system at
a holistic level using state machines to describe the desired
behavior~\cite{DiasNeto:2007:SMT:1353673.1353681}, and may or may not provide
fully automatic test-case generation. In addition to generating test-cases, many
model-based testing tools, \eg Spec Explorer~\cite{Veanes08} will produce extra artifacts
like visualizations to help the programmer understand the model. One could view
property-based testing, including our system, as a subset of model-based testing
focusing on lower-level properties of individual functions (unit-testing),
while using the type-structure of the functions under scrutiny to provide fully
automatic generation of test-cases.

\subsection{Property-based Testing}
\label{sec:property-based-testing}
Many property-based testing tools have been developed to automatically generate
test-suites. \quickcheck~\cite{claessen_quickcheck:_2000} randomly generates
inputs based on the property under scrutiny, but requires custom generators to
consistently generate constrained inputs. \cite{Claessen14Flops} extends
\quickcheck to randomly generate constrained values from a uniform distribution.
%
In contrast \smallcheck~\cite{runciman_smallcheck_2008} enumerates all possible
inputs up to some depth, which allows it to check existential properties in
addition to universal properties; however, it too has difficulty generating
inputs to properties with complex pre-conditions.
%
\lazysmallcheck~\cite{runciman_smallcheck_2008} addresses the issue of generating
constrained inputs by taking advantage of the inherent laziness of the
property, generating \emph{partially-defined} values (\ie values
containing $\bot$) and only filling in the holes if and when they are
demanded. 
%
Korat~\cite{Boyapati02} instruments a programmer-supplied
@repOk@ method, which checks class invariants and method pre-conditions, to
monitor which object fields are accessed. The authors observe that unaccessed fields
cannot have had an effect on the return value of @repOk@ and are thereby able to
exclude from the search space any objects that differ only in the values of the
unaccessed fields. 
%
While \lazysmallcheck and Korat's reliance on functions in the 
source language for specifying properties is convenient for the 
programmer (specification and implementation in the same language), 
it makes the method less amenable to formal verification, the
properties would need to be re-specified in another language 
that is restricted enough to facilitate verification.

\subsection{Symbolic Execution and Model-checking}
\label{sec:static-analysis}
Another popular technique for automatically generating test-cases is to analyze
the source code and attempt to construct inputs that will trigger different
paths through the program. DART~\cite{DART}, CUTE~\cite{CUTE}, 
and Pex~\cite{tillmann_pexwhite_2008} all use a
combination of symbolic and dynamic execution to explore different paths through
a program. 
%
While executing the program they collect \emph{path predicates},
conditions that characterize a path through a program, and at the end of a run
they negate the path predicates and query a constraint solver for another
assignment of values to program variables. This enables such tools to
efficiently explore many different paths through a program, but the technique
relies on the path predicates being expressible symbolically. When the
predicates are not expressible in the logic of the constraint solver, they fall
back to the values produced by the concrete execution, at a severe loss of
precision.
%
Instead of trying to trigger all paths through a program, one might 
simply try to trigger erroneous behavior. 
%
Check 'n' Crash~\cite{jcrasher} uses the ESC/Java analyzer~\cite{ESCJava} to discover
potential bugs and constructs concrete test-cases designed to trigger 
the bugs, if they exist. Similarly, \cite{ICSE04BLAST} uses the BLAST 
model-checker to construct test-cases that bring the program to 
a state satisfying some user-provided predicate.

In contrast to these approaches, \toolname (and more generally, property-based testing) 
treats the program as a \emph{black-box} and only requires that the pre- and 
post-conditions be expressible in the solver's logic. 
%
Of course, by expressing specifications in the source language, 
\eg as contracts, as in PEX~\cite{tillmann_pexwhite_2008}, one can use symbolic
execution to generate tests directly from specifications.
%
One concrete advantage of our approach over the symbolic execution based method
of PEX is that the latter generates tests by \emph{explicitly enumerating} paths
through the contract code, which suffers from a similar combinatorial 
problem as \smallcheck and \quickcheck. In contrast, \toolname performs the 
same search \emph{symbolically} within the SMT engine, which % we have demonstrated 
performs better for larger input sizes.


\subsection{Integrating Constraint-solving and Execution}
\label{sec:constraint-solving-execution}

\toolname is one of many tools that makes specifications 
executable via constraint solving. 
%
An early example of this approach is 
TestEra~\cite{Marinov:2001:TNF:872023.872551} 
that uses specifications written in the Alloy 
modeling language~\cite{jackson2002alloy} to 
generate all non-isomorphic Java objects that 
satisfy method pre-conditions and class invariants. 
%
As the specifications are written in Alloy, one can use 
Alloy's SAT-solver based model finding to symbolically 
enumerate candidate inputs.
%
Check 'n' Crash uses a similar idea, and SMT 
solvers to generate inputs that satisfy a given 
JML specification~\cite{jcrasher}.
%
Recent systems such as SBV~\cite{sbv} and 
Kaplan~\cite{Koksal:2012:CC:2103656.2103675} 
offer a monadic API for writing SMT constraints 
within the program, and use them to synthesize 
program values at \emph{run-time}. 
%
SBV provides a thin DSL over the logics understood 
by SMT solvers, whereas Kaplan integrates deeply 
with Scala, allowing the use of user-defined 
recursive types and functions. 
%
Test generation can be viewed as a special case 
of value-synthesis, and indeed Kaplan has been 
used to generate test-suites from preconditions 
in a similar manner to \toolname.

However, in all of the above (and also symbolic execution 
based methods like PEX or JCrasher), the specifications are 
\emph{assertions} in the Floyd-Hoare sense. 
%
Consequently, the techniques are limited to testing 
first-order functions over monomorphic data types.
%
In contrast, \toolname shows how to view \emph{types} as
executable specifications, which yields several advantages.
%
First, we can use types to compositionally lift specifications 
about flat values (\eg @Score@) over collections (\eg @[Score]@),
without requiring special recursive predicates to describe 
such collection invariants. 
% Similarly, we can use types to 
% smoothly lift specifications over arrow types, to test 
% higher-order functions.
%
Second, the compositional nature of types yields a 
compositional method for generating tests, allowing 
us to use type-class machinery to generate tests for
richer structures from tests for sub-structures.
%
Third, (refinement) types have proven to be effective 
for \emph{verifying} correctness properties in modern
modern languages that make ubiquitous use of parametric 
polymorphism and higher order 
functions~\cite{pfenningxi98,Dunfield07,SaraswatX10,fstar,VazouRealWorld14} 
and thus, we believe \toolname's approach of making refinement types
executable is a crucial step towards %
our goal of enabling 
\emph{gradual verification} for modern languages.

%%% BACKEND??? 
%%% That being said, Kaplan and SBV would be well-suited as a backend for \toolname.

%%%% NUKE SAID ELSEWHERE \subsection{Program Verification}
%%%% NUKE SAID ELSEWHERE \label{sec:program-verification}
%%%% NUKE SAID ELSEWHERE Many groups have worked on automatic verifiers that can prove 
%%%% NUKE SAID ELSEWHERE the absence of certain classes of bugs. Static contract 
%%%% NUKE SAID ELSEWHERE checkers~\cite{ESCJava,SpecSharp} use assertions and pre- and 
%%%% NUKE SAID ELSEWHERE post-conditions to verify program invariants. 
%%%% NUKE SAID ELSEWHERE %
%%%% NUKE SAID ELSEWHERE Our work builds on top of refinement type systems, which encode 
%%%% NUKE SAID ELSEWHERE complex invariants using predicates drawn from an SMT-decidable 
%%%% NUKE SAID ELSEWHERE logic~\cite{pfenningxi98}. 
%%%% NUKE SAID ELSEWHERE %
%%%% NUKE SAID ELSEWHERE The drawback to many of these systems is that the error messages 
%%%% NUKE SAID ELSEWHERE often become inscrutable, referring to temporary variables and 
%%%% NUKE SAID ELSEWHERE low-level constraints in the logic. 
%%%% NUKE SAID ELSEWHERE %
%%%% NUKE SAID ELSEWHERE In contrast a failing test-case provides the user with a concrete
%%%% NUKE SAID ELSEWHERE counter-example that he can examine to determine to source of 
%%%% NUKE SAID ELSEWHERE the error. 
%%%% NUKE SAID ELSEWHERE %
%%%% NUKE SAID ELSEWHERE We share a specification language with LiquidHaskell~\cite{VazouRealWorld14}, 
%%%% NUKE SAID ELSEWHERE providing the user with a seamless transition between testing 
%%%% NUKE SAID ELSEWHERE their program and proving it correct.

%%% REQUIRE auxiliary invariants -- e.g. RBTREE, balanced ---

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
