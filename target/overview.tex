\section{Overview}\label{sec:overview}

We start with a series of examples pertaining to a small grading
library called @Scores@. The examples provide a bird's eye view of 
how a user interacts with \toolname, how \toolname is implemented,
and the advantages of type-based testing.

\mypara{Refinement Types}
A refinement type is one where the basic types are decorated 
with logical predicates drawn from an efficiently decidable 
theory. For example,
%
\begin{code}
  type Nat   = {v:Int | 0 <= v}
  type Pos   = {v:Int | 0 <  v}
  type Rng N = {v:Int | 0 <= v && v <  N}
\end{code}
%
are refinement types describing the set of integers that are 
non-negative, strictly positive, and in the interval @[0, N)@ 
respectively. We will also build up function and collection 
types over base refinement types like the above. 
%
In this paper, we will not address the issue of \emph{checking}
refinement type signatures~\cite{VazouICFP14}.
%
We assume the code is typechecked, \eg by GHC, against the 
standard type signatures obtained by erasing the refinements.
Instead, we focus on using the refinements to 
synthesize tests to \emph{execute} the function, and to find 
\emph{counterexamples} that violate %demonstrate the function does not meet
the given specification.

\subsection{Testing with Types}

\mypara{Base Types}
Let us write a function @rescale@ that takes a source range @[0,r1)@, 
a target range @[0,r2)@, and a score @n@ from the source range,
and returns the linearly scaled score in the target range.
%
For example, @rescale 5 100 2@ should return @40@. 
Here is a first attempt at @rescale@ 
%
\begin{code}
  rescale :: r1:Nat -> r2:Nat -> s:Rng r1 -> Rng r2 
  rescale r1 r2 s = s * (r2 `div` r1)   
\end{code}
%
When we run \toolname, it immediately reports 
%
\begin{code}
  Found counter-example: (1, 0, 0) 
\end{code}
%
Indeed, @rescale 1 0 0@ results in @0@ which is not in the target 
@Rng 0@, as the latter is empty! We could fix this in various ways, 
\eg by requiring the ranges are non-empty:
%
\begin{code}
  rescale :: r1:Pos -> r2:Pos -> s:Rng r1 -> Rng r2 
\end{code}
%
Now, \toolname accepts the function and reports
%
\begin{code}
  OK. Passed all tests.
\end{code}
%
Thus, using the refinement type \emph{specification} for @rescale@, 
\toolname systematically tests the \emph{implementation} by generating 
all valid inputs (up to a given size bound) that respect the 
pre-conditions, running the function, and checking that the 
output satisfies the post-condition.
%
Testing against random, unconstrained inputs would be of limited value 
as the function is not designed to work on all @Int@ values. While in 
this case we could filter invalid inputs, we shall show
that \toolname can be more effective.

\mypara{Containers}
Let us suppose we have normalized all scores to be out of @100@
%
\begin{code}
  type Score = Rng 100
\end{code}
%
Next, let us write a function to compute a \emph{weighted} average 
of a list of scores.
%
\begin{code}
  average     :: [(Int, Score)] -> Score
  average []  = 0
  average wxs = total `div` n
    where
      total   = sum [w * x | (w, x) <- wxs ]
      n       = sum [w     | (w, _) <- wxs ]
\end{code}
%
It can be tricky to \emph{verify} this function as it requires non-linear reasoning
about an unbounded collection. However, we can gain a great degree of confidence by
systematically testing it using the type specification; indeed, \toolname responds:
%
\begin{code}
  Found counter-example: [(0,0)]
\end{code}
%
Clearly, an unfortunate choice of weights can trigger a divide-by-zero; we can fix 
this by requiring the weights be non-zero:
%
\begin{code}
  average :: [({v:Int | v /= 0}, Score)] -> Score
\end{code}
%
but now \toolname responds with
%
\begin{code}
  Found counter-example: [(-3,3),(3,0)]
\end{code}
% 
which also triggers the divide-by-zero! We will play it safe and require positive weights,
%
\begin{code}
  average :: [(Pos, Score)] -> Score
\end{code}
%
at which point \toolname reports that all tests pass.

\mypara{Ordered Containers}
The very nature of our business requires that at the end of the day,
we order students by their scores. We can represent ordered lists by 
requiring the elements of the tail @t@ to be greater than the head @h@:
%
\begin{code}
data OrdList a = [] | (:) {h :: a, t :: OrdList {v:a | h <= v}}
\end{code}
%
Note that erasing the refinement predicates gives us plain old Haskell lists.
We can now write a function to insert a score into an ordered list:
%
\begin{code}
  insert :: (Ord a) => a -> OrdList a -> OrdList a 
\end{code}
%
\toolname automatically generates all ordered lists (up to a given size)
and executes @insert@ to check for any errors. Unlike randomized testers, 
\toolname is not thwarted by the ordering constraint, and does not require a
custom generator from the user.

\mypara{Structured Containers} 
Everyone has a few bad days. Let us write a function that takes the 
@best k@ scores for a particular student. That is, the output
must satisfy a \emph{structural} constraint -- that its size 
equals @k@. We can encode the size of a list with a logical 
measure function~\cite{VazouICFP14}:
%
\begin{code}
  measure len :: [a] -> Nat
  len []      = 0
  len (x:xs)  = 1 + len xs
\end{code}
%
Now, we can stipulate that the output indeed has @k@ scores:
%
\begin{code}
  best      :: k:Nat -> [Score] -> {v:[Score] | k = len v}
  best k xs = take k $ reverse $ sort xs
\end{code}
%
Now, \toolname quickly finds a counterexample:
%
\begin{code}
  Found counter-example: (2,[])
\end{code}
%
Of course -- we need to have at least @k@ scores to start with! 
%
\begin{code}
best :: k:Nat -> {v:[Score]|k <= len v} -> {v:[Score]|k = len v}
\end{code}
%
and now, \toolname is assuaged and reports no counterexamples.
%
While randomized testing would suffice for @best@, we will see 
more sophisticated structural properties such as height balancedness, 
which stymie random testers, but are easily handled by \toolname.

\mypara{Higher-order Functions} 
Perhaps instead of taking the $k$ best grades, we would like
to pad each individual grade, and, furthermore, we want to
be able to experiment with different padding functions. Let
us rewrite @average@ to take a functional argument, and
stipulate that it can only increase a @Score@.
%
\begin{code}
  padAverage       :: (s:Score -> {v:Score | s <= v}) 
                   -> [(Pos, Score)] -> Score
  padAverage f []  = f 0
  padAverage f wxs = total `div` n
    where
      total   = sum [w * f x | (w, x) <- wxs ]
      n       = sum [w       | (w, _) <- wxs ]
\end{code}
%
\toolname automatically checks that @padAverage@ is 
a safe generalization of @average@. Randomized 
testing tools can also generate functions, but those 
functions are unlikely to satisfy non-trivial constraints, 
thereby burdening the user with custom generators.


\subsection{Synthesizing Tests} 
\label{sec:synthesizing-tests}
Next, let us look under the hood to get an idea of how \toolname 
synthesizes tests from types. 
% INTRO
At a high-level, our strategy is to:
%
(1)~\emph{query}   an SMT solver for satisfying assigments to a set of logical 
                   constraints derived from the refinement type,
(2)~\emph{decode}  the model into Haskell values that are suitable inputs,
(3)~\emph{execute} the function on the decoded values to obtain the output, 
(4)~\emph{check}   that the output satisfies the output type,
(5)~\emph{refute}  the model to generate a different test, and 
repeat the above steps until all tests up to a certain size are executed.
%
We focus here on steps 1, 2, and 4 -- query, decode, and check -- the others are 
standard and require little explanation.

\mypara{Base Types}
Recall the initial (buggy) specification
%
\begin{code}
  rescale :: r1:Nat -> r2:Nat -> s:Rng r1 -> Rng r2 
\end{code}
%
\toolname \emph{encodes} input requirements for base types directly 
from their corresponding refinements. The constraints for multiple, 
related inputs are just the \emph{conjunction} of the constraints 
for each input. Hence, the constraint for @rescale@ is:
%
$$
\cstr{C_0} \defeq 0 \leq \cvar{r1} \wedge 0 \leq \cvar{r2} \wedge 0 \leq s < \cvar{r1} 
$$
%
In practice, $\cstr{C_0}$ will also contain conjuncts of the form $-N \leq x \leq N$ that
restrict @Int@-valued variables $x$ to be within the size bound $N$ supplied by
the user, but we will omit these throughout the paper for clarity.
%% %
%% For clarity, we omit the conjuncts of the form $-N \leq x \leq N$
%% that restrict @Int@-valued variables $x$ to be within the size
%% bound $N$ supplied by the user.

Note how easy it is to capture dependencies between inputs, 
\eg that the score @s@ be in the range defined by @r1@.
%
On querying the SMT solver with the above, we get a model
%
$[\cvar{r1} \mapsto 1, \cvar{r2} \mapsto 1, \cvar{s}  \mapsto 0]$.
%
\toolname decodes this model and executes \hbox{@rescale 1 1 0@} to obtain the value @v = 0@.
%
Then, \toolname validates @v@ against the post-condition by checking 
% that it inhabits the output type, \ie by checking 
the validity of the output type's constraint: 
%
$$\cvar{r2} = 1 \wedge \cvar{v} = 0 \wedge 0 \leq \cvar{v} \wedge \cvar{v} < \cvar{r2}$$
%
As the above is valid, \toolname moves on to generate another 
test by conjoining $\cstr{C_0}$ with a constraint that refutes 
the previous model:
%
$$
\cstr{C_1} \defeq \cstr{C_0} \wedge (\cvar{r1} \not = 1 \vee \cvar{r2} \not = 1 \vee \cvar{s} \not = 0)
$$
This time, the SMT solver returns a model: 
%
$[\cvar{r1} \mapsto 1, \cvar{r2} \mapsto 0, \cvar{s} \mapsto 0]$
%
which, when decoded and executed, yields the result $0$ that does \emph{not} 
inhabit the output type, and so is reported as a counterexample. 
%
When we fix the specification to only allow @Pos@ ranges, each test produces
a valid output, so \toolname reports that all tests pass.

\mypara{Containers}
Next, we use \toolname to test the implementation of @average@.
To do so, \toolname needs to generate Haskell lists with the appropriate constraints.
%
Since each list is recursively 
either ``nil'' 
or ``cons'', 
\toolname generates constraints that symbolically 
represent \emph{all} possible lists up to a given depth, 
using propositional \emph{choice variables} to 
symbolically pick between these two alternatives.
%
Every (satisfying) assignment of choices returned by 
the SMT solver gives \toolname the concrete data and 
constructors used at each level, allowing it to decode 
the assignment into a Haskell value.

For example, \toolname represents valid @[(Pos, Score)]@ 
inputs (of depth up to 3), required to test @average@, 
as the conjunction of $\cstr{C_{list}}$ and $\cstr{C_{data}}$:
%
\begin{eqnarray*}
\cstr{C_{list}} & \defeq & (\cvar{c}_{00} \Rightarrow \cvar{xs}_0 = \lnil) \wedge 
                          (\cvar{c}_{01} \Rightarrow \cvar{xs}_0 = \lcons{\cvar{x}_1}{\cvar{xs}_1}) \wedge 
                          (\cvar{c}_{00} \oplus \cvar{c}_{01}) \\
               & \wedge & (\cvar{c}_{10} \Rightarrow \cvar{xs}_1 = \lnil) \wedge
                          (\cvar{c}_{11} \Rightarrow \cvar{xs}_1 = \lcons{\cvar{x}_2}{\cvar{xs}_2}) \wedge 
                          (\cvar{c}_{01} \Rightarrow \cvar{c}_{10} \oplus \cvar{c}_{11}) \\
               & \wedge & (\cvar{c}_{20} \Rightarrow \cvar{xs}_2 = \lnil) \wedge 
                          (\cvar{c}_{21} \Rightarrow \cvar{xs}_2 = \lcons{\cvar{x}_3}{\cvar{xs}_3}) \wedge 
                          (\cvar{c}_{11} \Rightarrow \cvar{c}_{20} \oplus \cvar{c}_{21}) \\
               & \wedge & (\cvar{c}_{30} \Rightarrow \cvar{xs}_3 = \lnil) \wedge 
                          (\cvar{c}_{21} \Rightarrow \cvar{c}_{30}) \\[0.1in]
\cstr{C_{data}} & \defeq & (\cvar{c}_{01} \Rightarrow \cvar{x}_1 = \ltup{\cvar{w}_1}{\cvar{s}_1} \ \wedge\ 0 < \cvar{w}_1 \ \wedge\ 0 \leq \cvar{s}_1 < 100) \\
               & \wedge & (\cvar{c}_{11} \Rightarrow \cvar{x}_2 = \ltup{\cvar{w}_2}{\cvar{s}_2} \ \wedge\ 0 < \cvar{w}_2 \ \wedge\ 0 \leq \cvar{s}_2 < 100) \\
               & \wedge & (\cvar{c}_{21} \Rightarrow \cvar{x}_3 = \ltup{\cvar{w}_3}{\cvar{s}_3} \ \wedge\ 0 < \cvar{w}_3 \ \wedge\ 0 \leq \cvar{s}_3 < 100)
\end{eqnarray*}
%
The first set of constraints $\cstr{C_{list}}$ describes all lists up to 
size 3. At each level $i$, the \emph{choice} variables $\cvar{c}_{i0}$ 
and $\cvar{c}_{i1}$ determine whether at that level the constructed 
list $\cvar{xs}_i$ is a ``nil'' or a ``cons''. 
%
In the constraints $\lnil$ and $(\lcons{}{})$ are \emph{uninterpreted} 
functions that represent ``nil'' and ``cons'' respectively. 
These functions only obey the congruence axiom and hence, can be 
efficiently analyzed by SMT solvers~\cite{Nelson81}.
%
The data at each level $\cvar{x}_i$ is constrained to be a pair of a 
positive weight $\cvar{w}_i$ and a valid score $\cvar{s}_i$.

The choice variables at each level are used to \emph{guard} the 
constraints on the next levels. 
%
First, if we are generating a ``cons'' at a given level, then
exactly one of the choice variables for the next level must be 
selected;
\eg  $\cvar{c}_{11} \Rightarrow \cvar{c}_{20} \oplus \cvar{c}_{21}$.
%
Second, the constraints on the data at a given level only hold 
if we are generating values for that level; \eg $\cvar{c}_{21}$ 
is used to guard the constraints on $\cvar{x}_3$, $\cvar{w}_3$ 
and $\cvar{s}_3$.
%
This is essential to avoid over-constraining the system 
which would cause \toolname to miss certain tests.

To \emph{decode} a model of the above into a Haskell value of type @[(Int, Int)]@,
we traverse constraints and use the valuations of the choice variables to 
build up the list appropriately.
%
At each level, if $\cvar{c}_{i0} \mapsto \ttrue$, then the list at that 
level is @[]@, otherwise $\cvar{c}_{i1} \mapsto \ttrue$ and we decode 
$\cvar{x}_{i+1}$ and $\cvar{xs}_{i+1}$ and ``cons'' the results.

We can iteratively generate \emph{multiple} inputs by adding a constraint that
refutes each prior model. As an important optimization, we only refute the
relevant parts of the model, \ie those needed to construct the list
(\S~\ref{sec:refute}).

\mypara{Ordered Containers}
%
Next, let us see how \toolname enables automatic testing with 
highly constrained inputs, such as the \emph{increasingly ordered} 
@OrdList@ values required by @insert@.
%
From the type definition, it is apparent that ordered
lists are the same as the usual lists described by
$\cstr{C_{list}}$, except that each unfolded \emph{tail} 
must only contain values that are greater than the 
corresponding \emph{head}.
%
That is, as we unfold @x1:x2:xs :: OrdList@ 
%
\begin{itemize}
\item At level @0@, we have @OrdList {v:Score| true}@
\item At level @1@, we have @OrdList {v:Score| x1 <= v}@
\item At level @2@, we have @OrdList {v:Score| x2 <= v && x1 <= v}@
\end{itemize}
%
and so on. Thus, we encode @OrdList Score@ (of depth up to 3) by
conjoining $\cstr{C_{list}}$ with  $\cstr{C_{score}}$ and $\cstr{C_{ord}}$,
which capture the valid score and ordering requirements respectively:
%
\begin{eqnarray*}
\cstr{C_{ord}}   & \defeq & (\cvar{c}_{11} \Rightarrow \cvar{x}_1 \leq \cvar{x}_2)
                \ \wedge\  (\cvar{c}_{21} \Rightarrow \cvar{x}_2 \leq \cvar{x}_3\ \wedge\ \cvar{x}_1 \leq \cvar{x}_3) \\[0.01in]
\cstr{C_{score}} & \defeq & (\cvar{c}_{01} \Rightarrow 0 \leq \cvar{x}_1 < 100)
                \ \wedge\  (\cvar{c}_{11} \Rightarrow 0 \leq \cvar{x}_2 < 100)
                \ \wedge\  (\cvar{c}_{21} \Rightarrow 0 \leq \cvar{x}_3 < 100)
\end{eqnarray*}

\mypara{Structured Containers}
Recall that @best k@ requires inputs whose \emph{structure} is constrained -- the 
size of the list should be no less than @k@. We specify size using special measure 
functions~\cite{VazouICFP14}, which let us relate the size of a list with that of
its unfolding, and hence, let us encode the notion of size inside the constraints:
%
\begin{eqnarray*}
\cstr{C_{size}} & \defeq & (\cvar{c}_{00} \Rightarrow \clen{\cvar{xs}_{0}} = 0) \wedge 
                          (\cvar{c}_{01} \Rightarrow \clen{\cvar{xs}_{0}} = 1 + \clen{\cvar{xs}_1}) \\
               & \wedge & (\cvar{c}_{10} \Rightarrow \clen{\cvar{xs}_{1}} = 0) \wedge 
                          (\cvar{c}_{11} \Rightarrow \clen{\cvar{xs}_{1}} = 1 + \clen{\cvar{xs}_2}) \\
               & \wedge & (\cvar{c}_{20} \Rightarrow \clen{\cvar{xs}_{2}} = 0) \wedge 
                          (\cvar{c}_{21} \Rightarrow \clen{\cvar{xs}_{2}} = 1 + \clen{\cvar{xs}_3}) \\
               & \wedge & (\cvar{c}_{30} \Rightarrow \clen{\cvar{xs}_{3}} = 0)
\end{eqnarray*}
%
At each unfolding, we instantiate the definition of the measure 
for each alternative of the datatype. 
%
In the constraints, $\clen{\cdot}$ is an uninterpreted function derived
from the measure definition. All of the relevant properties of the function
are spelled out by the unfolded constraints in $\cstr{C_{size}}$ and hence,
we can use SMT to search for models for the above constraint.
%
Hence, \toolname constrains the input type for @best@ as:
%
$$     0 \leq k 
\wedge \cstr{C_{list}} 
\wedge \cstr{C_{score}} 
\wedge \cstr{C_{size}} 
\wedge k \leq \clen{\cvar{xs}_0} $$
%
where the final conjunct comes from the top-level refinement that 
stipulates the input have at least @k@ scores.
%
Thus, \toolname only generates lists that are large enough. 
For example, in any model where $k = 2$, it will \emph{not} 
generate the empty or singleton list, as in those cases, 
$\clen{\cvar{xs}_0}$ would be $0$ (resp. $1$), violating the 
final conjunct above.

\mypara{Higher-order Functions}
Finally, \toolname's type-directed testing scales up to higher-order
functions using the same insight as in QuickCheck~\cite{claessen_quickcheck:_2000}, namely, 
to generate a function it suffices to be able to 
generate the \emph{output} of the function.
When tasked with the generation of a functional argument @f@, \toolname 
returns a Haskell function that when executed checks
whether its inputs satisfy @f@'s pre-conditions.
If they do, then @f@ uses \toolname to dynamically
query the SMT solver for an output that satisfies the 
constraints imposed by the concrete inputs.
Otherwise, @f@'s specifications are violated
and TARGET reports a counterexample.

This concludes our high-level tour of the benefits and 
implementation of \toolname. 
%
Notice that the property specification mechanism -- 
refinement types -- allowed us to get immediate feedback
that helped debug not just the code, but also the specification 
itself. 
%
Additionally, the specifications gave us machine-readable 
documentation about the behavior of functions, and a large 
unit test suite with which to automatically validate the 
implementation.
%
Finally, though we do not focus on it here, the specifications 
are amenable to formal verification should the programmer 
so desire.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
